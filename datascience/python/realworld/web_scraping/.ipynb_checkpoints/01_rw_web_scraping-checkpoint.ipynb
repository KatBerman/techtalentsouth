{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science Fundamentals: Python |\n",
    "[Table of Contents](../../index.ipynb)\n",
    "- - - \n",
    "<!--NAVIGATION-->\n",
    "Real World Examples: **[Web Scraping](./01_rw_web_scraping.ipynb)** | [Automation](../automation/02_rw_automation.ipynb) | [Messaging](../messaging/03_rw_messaging.ipynb) | [CSV](../csv/04_rw_csv.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real World: Scrape Data from nearly Any Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Congressional database that we’re using is not an easy one to scrape because the URL for the search results remains the same regardless of what you’re searching for. While this can be bypassed programmatically, it is easier for our purposes to go to this link of [Biographical Directory of the United States Congress](https://bioguideretro.congress.gov/) and do a search for the 43rd year of our congress.  Then save a copy of the page with a SAVE AS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Congress](files/congress.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install BeautifulSoup, LXML and Future"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error from Command Line you might want to submit this line "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sudo pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error from Command Line you might want to submit this line"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sudo pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that this code works with either Python2 or Python3, you will need one helper library. Run in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    " \n",
    "soup = BeautifulSoup(\"<html><p>This is <b>invalid HTML</p></html>\", \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting names and URLs from an HTML page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be attempting to go from a search results page where the html page looks like this:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<table border=\"1\" cellspacing=\"2\" cellpadding=\"3\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<th>Member Name</th>\n",
    "<th>Birth-Death</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035\">ADAMS, George Madison</a></td>\n",
    "<td>1837-1920</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074\">ALBERT, William Julian</a></td>\n",
    "<td>1816-1879</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077\">ALBRIGHT, Charles</a></td>\n",
    "<td>1830-1880</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To a CSV file that looks like this -"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"ADAMS, George Madison\",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035\n",
    "\"ALBERT, William Julian\",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074\n",
    "\"ALBRIGHT, Charles\",http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, import the Beautiful Soup library, open the HTML file and pass it to Beautiful Soup, and then print the “pretty” version in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <body>\n",
      "  <table border=\"1\" cellpadding=\"3\" cellspacing=\"2\">\n",
      "   <tbody>\n",
      "    <tr>\n",
      "     <th>\n",
      "      Member Name\n",
      "     </th>\n",
      "     <th>\n",
      "      Birth-Death\n",
      "     </th>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td>\n",
      "      <a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035\">\n",
      "       ADAMS, George Madison\n",
      "      </a>\n",
      "     </td>\n",
      "     <td>\n",
      "      1837-1920\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td>\n",
      "      <a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074\">\n",
      "       ALBERT, William Julian\n",
      "      </a>\n",
      "     </td>\n",
      "     <td>\n",
      "      1816-1879\n",
      "     </td>\n",
      "    </tr>\n",
      "    <tr>\n",
      "     <td>\n",
      "      <a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077\">\n",
      "       ALBRIGHT, Charles\n",
      "      </a>\n",
      "     </td>\n",
      "     <td>\n",
      "      1830-1880\n",
      "     </td>\n",
      "    </tr>\n",
      "   </tbody>\n",
      "  </table>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(open(\"files/43rd-congress.html\"), features=\"lxml\")\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the names and the URLs are, most fortunately, embedded in the tags. So, we need to isolate out all of the tags. We can do this by updating the code -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000035\">ADAMS, George Madison</a>\n",
      "<a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000074\">ALBERT, William Julian</a>\n",
      "<a href=\"http://bioguide.congress.gov/scripts/biodisplay.pl?index=A000077\">ALBRIGHT, Charles</a>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup (open(\"files/43rd-congress.html\"), features=\"lxml\")\n",
    "\n",
    "# print(soup.prettify())\n",
    "\n",
    "links = soup.find_all('a')\n",
    "\n",
    "for link in links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following code is broken to complete the task.  Can you troubleshoot and correct the code?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '43rd-congress.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db5b1cdab2f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"43rd-congress.html\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(soup.prettify())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '43rd-congress.html'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "soup = BeautifulSoup (open(\"files/43rd-congress.html\"), features=\"lxml\")\n",
    "\n",
    "# print(soup.prettify())\n",
    "\n",
    "final_link = soup.p.a\n",
    "final_link.decompose()\n",
    "\n",
    "f= csv.writer(open(\"files/43rd_Congress_all.csv\", \"w\"))   # Open the output file for writing before the loop\n",
    "f.writerow([\"Name\", \"Years\", \"Position\", \"Party\", \"State\", \"Congress\", \"Link\"]) # Write column headers as the first line\n",
    "\n",
    "trs = soup.find_all('tr')\n",
    "\n",
    "for tr in trs:\n",
    "    for link in tr.find_all('a'):\n",
    "        fullLink = link.get ('href')\n",
    "\n",
    "    tds = tr.find_all(\"td\")\n",
    "\n",
    "    try: #we are using \"try\" because the table is not well formatted. This allows the program to continue after encountering an error.\n",
    "        names = str(tds[0].get_text()) # This structure isolate the item by its column in the table and converts it into a string.\n",
    "        years = str(tds[1].get_text())\n",
    "        positions = str(tds[2].get_text())\n",
    "        parties = str(tds[3].get_text())\n",
    "        states = str(tds[4].get_text())\n",
    "        congress = tds[5].get_text()\n",
    "\n",
    "    except:\n",
    "        print(\"bad tr string: {}\".format(tds))\n",
    "        continue #This tells the computer to move on to the next item after it encounters an error\n",
    "\n",
    "    f.writerow([names, years, positions, parties, states, congress, fullLink])\n",
    "\n",
    "# f = csv.writer(open(\"43rd_Congress.csv\", \"w\"))\n",
    "# f.writerow([\"Name\", \"Link\"])    # Write column headers as the first line\n",
    "\n",
    "# links = soup.find_all('a')\n",
    "# for link in links:\n",
    "#     names = link.contents[0]\n",
    "#     fullLink = link.get('href')\n",
    "#     # print(names)\n",
    "#     # print(fullLink)\n",
    "\n",
    "#     f.writerow([names,fullLink])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - \n",
    "<!--NAVIGATION-->\n",
    "Real World Examples: **[Web Scraping](./01_rw_web_scraping.ipynb)** | [Automation](../automation/02_rw_automation.ipynb) | [Messaging](../messaging/03_rw_messaging.ipynb) | [CSV](../csv/04_rw_csv.ipynb) \n",
    "<br>\n",
    "[Top](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - -\n",
    "\n",
    "Copyright © 2020 Qualex Consulting Services Incorporated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
